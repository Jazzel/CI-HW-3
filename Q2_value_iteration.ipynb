{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [],
   "source": [
    "import numpy as np\n",
    "import gym\n",
    "import random\n",
    "import math"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "array([[[180, 200, 230],\n",
       "        [180, 200, 230],\n",
       "        [180, 200, 230],\n",
       "        ...,\n",
       "        [180, 200, 230],\n",
       "        [180, 200, 230],\n",
       "        [180, 200, 230]],\n",
       "\n",
       "       [[180, 200, 230],\n",
       "        [204, 230, 255],\n",
       "        [204, 230, 255],\n",
       "        ...,\n",
       "        [204, 230, 255],\n",
       "        [204, 230, 255],\n",
       "        [180, 200, 230]],\n",
       "\n",
       "       [[180, 200, 230],\n",
       "        [235, 245, 249],\n",
       "        [204, 230, 255],\n",
       "        ...,\n",
       "        [204, 230, 255],\n",
       "        [204, 230, 255],\n",
       "        [180, 200, 230]],\n",
       "\n",
       "       ...,\n",
       "\n",
       "       [[180, 200, 230],\n",
       "        [235, 245, 249],\n",
       "        [235, 245, 249],\n",
       "        ...,\n",
       "        [204, 230, 255],\n",
       "        [235, 245, 249],\n",
       "        [180, 200, 230]],\n",
       "\n",
       "       [[180, 200, 230],\n",
       "        [235, 245, 249],\n",
       "        [235, 245, 249],\n",
       "        ...,\n",
       "        [204, 230, 255],\n",
       "        [204, 230, 255],\n",
       "        [180, 200, 230]],\n",
       "\n",
       "       [[180, 200, 230],\n",
       "        [180, 200, 230],\n",
       "        [180, 200, 230],\n",
       "        ...,\n",
       "        [180, 200, 230],\n",
       "        [180, 200, 230],\n",
       "        [180, 200, 230]]], dtype=uint8)"
      ]
     },
     "execution_count": 16,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "env1 =[\"SHFF\", \"FFFH\", \"FHFH\", \"HFFG\"]\n",
    "env2= [\"SFFFFF\", \"FFFHFF\", \"FHFHHH\", \"HFFFFG\"]\n",
    "env3 = ['SFFHFFHH', 'HFFFFFHF', 'HFFHHFHH', 'HFHHHFFF', 'HFHHFHFF', 'FFFFFFFH', 'FHHFHFHH', 'FHHFHFFG'] \n",
    "\n",
    "selectedEnv = env2\n",
    "env = gym.make('FrozenLake-v1', desc=selectedEnv, render_mode=\"rgb_array\", is_slippery = False)\n",
    "env.reset()\n",
    "env.render()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [],
   "source": [
    "# change-able parameters:\n",
    "discount_factor = 0.99\n",
    "delta_threshold = 0.00001\n",
    "epsilon = 1"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "metadata": {},
   "outputs": [],
   "source": [
    "def value_iteration(env, gamma=0.9, epsilon=1e-6):\n",
    "    num_states = env.observation_space.n\n",
    "    num_actions = env.action_space.n\n",
    "\n",
    "    # Initialize the value function for all states as 0\n",
    "    V = np.zeros(num_states)\n",
    "    \n",
    "    # Learning the policy here\n",
    "    while True:\n",
    "        delta = 0\n",
    "        for state in range(num_states):\n",
    "            v = V[state] #value of the current state\n",
    "            # Applying the Bellman equation to update the value of the state\n",
    "            temp_values = np.zeros(num_actions) \n",
    "            for action in range(num_actions): #for each action\n",
    "                for prob, next_state, reward, _ in env.P[state][action]: #for each possible transition\n",
    "                    temp_values[action] += prob * (reward + gamma * V[next_state]) #summation of the expected value of the next state\n",
    "             #Updating the value of the state by choosing action that maximizes the expected value of the next state\n",
    "            V[state] = np.max(temp_values)\n",
    "            delta = max(delta, np.abs(v - V[state])) #calculating the change in value of the state\n",
    "        if delta < epsilon: #if the change in value of the state is less than the threshold the optimal plocy is learnt\n",
    "            break\n",
    "\n",
    "    #Code for extracting optimal policy from the value function\n",
    "\n",
    "    policy = np.zeros(num_states)   #initializing the policy\n",
    "    for state in range(num_states): #for each state\n",
    "        temp_values = np.zeros(num_actions) \n",
    "        for action in range(num_actions):  #for each action\n",
    "            for prob, next_state, reward,_ in env.P[state][action]: #for each possible transition\n",
    "                temp_values[action] += prob * (reward + gamma * V[next_state]) #summation of the expected value of the next state\n",
    "        policy[state] = np.argmax(temp_values) #selecting the action that maximizes the expected value of the next state\n",
    "    \n",
    "    return policy, V"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Optimal Value Function:\n",
      "[[0.93206535 0.94148015 0.95099005 0.94148015 0.93206535 0.92274469]\n",
      " [0.94148015 0.95099005 0.96059601 0.         0.92274469 0.91351725]\n",
      " [0.93206535 0.         0.970299   0.         0.         0.        ]\n",
      " [0.         0.970299   0.9801     0.99       1.         0.        ]]\n",
      "\n",
      "Optimal Policy (0=Left, 1=Down, 2=Right, 3=Up):\n",
      "[[1. 1. 1. 0. 0. 0.]\n",
      " [2. 2. 1. 0. 3. 0.]\n",
      " [3. 0. 1. 0. 0. 0.]\n",
      " [0. 2. 2. 2. 2. 0.]]\n"
     ]
    }
   ],
   "source": [
    "# Run value iteration\n",
    "policy, V = value_iteration(env, gamma=discount_factor, epsilon=delta_threshold)\n",
    "\n",
    "# Print results\n",
    "print(\"Optimal Value Function:\")\n",
    "print(V.reshape(len(selectedEnv), len(selectedEnv[0])))\n",
    "\n",
    "print(\"\\nOptimal Policy (0=Left, 1=Down, 2=Right, 3=Up):\")\n",
    "print(policy.reshape(len(selectedEnv), len(selectedEnv[0])))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "0\n",
      "number of steps taken: 7\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "c:\\Users\\HP\\miniconda3\\envs\\env\\Lib\\site-packages\\gym\\utils\\passive_env_checker.py:233: DeprecationWarning: `np.bool8` is a deprecated alias for `np.bool_`.  (Deprecated NumPy 1.24)\n",
      "  if not isinstance(terminated, (bool, np.bool8)):\n"
     ]
    }
   ],
   "source": [
    "# resetting the environment and executing the policy\n",
    "state = env.reset()\n",
    "state = state[0]\n",
    "step = 0\n",
    "done = False\n",
    "print(state)\n",
    "\n",
    "max_steps = 100\n",
    "for step in range(max_steps):\n",
    "\n",
    "    # Getting max value against that state, so that we choose that action\n",
    "    action = policy[state]\n",
    "    new_state, reward, done, truncated, info = env.step(action) #information after taking the action\n",
    "    env.render()\n",
    "    if done:\n",
    "        print(\"number of steps taken:\", step)\n",
    "        break\n",
    "\n",
    "    state = new_state\n",
    "\n",
    "env.close()\n"
   ]
  }
 ],
 "metadata": {
  "interpreter": {
   "hash": "b6e66f5b7dd30519e522d238cc77648fcf0517b95e3943f8a24c96d98e75c6c1"
  },
  "kernelspec": {
   "display_name": "Python 3.9.2 64-bit",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.12.3"
  },
  "orig_nbformat": 4
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
