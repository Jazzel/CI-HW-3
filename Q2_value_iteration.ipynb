{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [],
   "source": [
    "import numpy as np\n",
    "import gym\n",
    "import random\n",
    "import math"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "array([[[180, 200, 230],\n",
       "        [180, 200, 230],\n",
       "        [180, 200, 230],\n",
       "        ...,\n",
       "        [180, 200, 230],\n",
       "        [180, 200, 230],\n",
       "        [180, 200, 230]],\n",
       "\n",
       "       [[180, 200, 230],\n",
       "        [204, 230, 255],\n",
       "        [204, 230, 255],\n",
       "        ...,\n",
       "        [204, 230, 255],\n",
       "        [204, 230, 255],\n",
       "        [180, 200, 230]],\n",
       "\n",
       "       [[180, 200, 230],\n",
       "        [235, 245, 249],\n",
       "        [204, 230, 255],\n",
       "        ...,\n",
       "        [204, 230, 255],\n",
       "        [204, 230, 255],\n",
       "        [180, 200, 230]],\n",
       "\n",
       "       ...,\n",
       "\n",
       "       [[180, 200, 230],\n",
       "        [235, 245, 249],\n",
       "        [235, 245, 249],\n",
       "        ...,\n",
       "        [204, 230, 255],\n",
       "        [235, 245, 249],\n",
       "        [180, 200, 230]],\n",
       "\n",
       "       [[180, 200, 230],\n",
       "        [235, 245, 249],\n",
       "        [235, 245, 249],\n",
       "        ...,\n",
       "        [204, 230, 255],\n",
       "        [204, 230, 255],\n",
       "        [180, 200, 230]],\n",
       "\n",
       "       [[180, 200, 230],\n",
       "        [180, 200, 230],\n",
       "        [180, 200, 230],\n",
       "        ...,\n",
       "        [180, 200, 230],\n",
       "        [180, 200, 230],\n",
       "        [180, 200, 230]]], dtype=uint8)"
      ]
     },
     "execution_count": 2,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "env1 =[\"SHFF\", \"FFFH\", \"FHFH\", \"HFFG\"]\n",
    "env2= [\"SFFFFF\", \"FFFHFF\", \"FHFHHH\", \"HFFFFG\"]\n",
    "env3 = ['SFFHFFHH', 'HFFFFFHF', 'HFFHHFHH', 'HFHHHFFF', 'HFHHFHFF', 'FFFFFFFH', 'FHHFHFHH', 'FHHFHFFG'] \n",
    "\n",
    "selectedEnv = env2\n",
    "env = gym.make('FrozenLake-v1', desc=selectedEnv, render_mode=\"rgb_array\", is_slippery = False)\n",
    "env.reset()\n",
    "env.render()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [],
   "source": [
    "# change-able parameters:\n",
    "discount_factor = 0.99\n",
    "delta_threshold = 0.00001\n",
    "epsilon = 1"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [],
   "source": [
    "def value_iteration(env, gamma=0.9, epsilon=1e-6):\n",
    "    num_states = env.observation_space.n\n",
    "    num_actions = env.action_space.n\n",
    "\n",
    "    # Initialize the value function\n",
    "    V = np.zeros(num_states)\n",
    "\n",
    "    #Write your code to implement value iteration main loop\n",
    "    while True:\n",
    "        delta = 0\n",
    "        for state in range(num_states):\n",
    "            v = V[state]\n",
    "            q_values = np.zeros(num_actions)\n",
    "            for action in range(num_actions):\n",
    "                for prob, next_state, reward, done in env.P[state][action]:\n",
    "                    q_values[action] += prob * (reward + gamma * V[next_state])\n",
    "            V[state] = np.max(q_values)\n",
    "            delta = max(delta, np.abs(v - V[state]))\n",
    "        if delta < epsilon:\n",
    "            break\n",
    "\n",
    "         \n",
    "    # Write your code here to extract the optimal policy from value function. \n",
    "    # For each state, the policy will tell you the action to take\n",
    "    policy = np.zeros(num_states, dtype=int)\n",
    "    for state in range(num_states):\n",
    "        q_values = np.zeros(num_actions)\n",
    "        for action in range(num_actions):\n",
    "            for prob, next_state, reward, done in env.P[state][action]:\n",
    "                q_values[action] += prob * (reward + gamma * V[next_state])\n",
    "        policy[state] = np.argmax(q_values)\n",
    "    \n",
    "       \n",
    "    return policy, V"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Optimal Value Function:\n",
      "[[0.4782969  0.531441   0.59049    0.531441   0.4782969  0.43046721]\n",
      " [0.531441   0.59049    0.6561     0.         0.43046721 0.38742049]\n",
      " [0.4782969  0.         0.729      0.         0.         0.        ]\n",
      " [0.         0.729      0.81       0.9        1.         0.        ]]\n",
      "\n",
      "Optimal Policy (0=Left, 1=Down, 2=Right, 3=Up):\n",
      "[[1 1 1 0 0 0]\n",
      " [2 2 1 0 3 0]\n",
      " [3 0 1 0 0 0]\n",
      " [0 2 2 2 2 0]]\n"
     ]
    }
   ],
   "source": [
    "# Run value iteration\n",
    "policy, V = value_iteration(env)\n",
    "\n",
    "# Print results\n",
    "print(\"Optimal Value Function:\")\n",
    "print(V.reshape(len(selectedEnv), len(selectedEnv[0])))\n",
    "\n",
    "print(\"\\nOptimal Policy (0=Left, 1=Down, 2=Right, 3=Up):\")\n",
    "print(policy.reshape(len(selectedEnv), len(selectedEnv[0])))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "0\n",
      "number of steps taken: 7\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "e:\\HU\\CI\\CI-HW-3\\env\\Lib\\site-packages\\gym\\utils\\passive_env_checker.py:233: DeprecationWarning: `np.bool8` is a deprecated alias for `np.bool_`.  (Deprecated NumPy 1.24)\n",
      "  if not isinstance(terminated, (bool, np.bool8)):\n"
     ]
    }
   ],
   "source": [
    "# resetting the environment and executing the policy\n",
    "state = env.reset()\n",
    "state = state[0]\n",
    "step = 0\n",
    "done = False\n",
    "print(state)\n",
    "\n",
    "max_steps = 100\n",
    "for step in range(max_steps):\n",
    "\n",
    "    # Getting max value against that state, so that we choose that action\n",
    "    action = policy[state]\n",
    "    new_state, reward, done, truncated, info = env.step(action) #information after taking the action\n",
    "\n",
    "    env.render()\n",
    "    if done:\n",
    "        print(\"number of steps taken:\", step)\n",
    "        break\n",
    "\n",
    "\n",
    "    state = new_state\n",
    "\n",
    "env.close()\n",
    "\n"
   ]
  }
 ],
 "metadata": {
  "interpreter": {
   "hash": "b6e66f5b7dd30519e522d238cc77648fcf0517b95e3943f8a24c96d98e75c6c1"
  },
  "kernelspec": {
   "display_name": "Python 3.9.2 64-bit",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.12.1"
  },
  "orig_nbformat": 4
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
